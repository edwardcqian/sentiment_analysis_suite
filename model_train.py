import sys
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score
import pickle, os

from keras.models import Model
from keras.layers import Dense, Embedding, Input, Activation, Conv1D, Flatten, MaxPooling1D, Add, concatenate, SpatialDropout1D
from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout, CuDNNLSTM, CuDNNGRU, GlobalAveragePooling1D
from keras.preprocessing import text, sequence
from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback

import lightgbm as lgb

from utils import clean_text
from utils import re_sample
from utils import pr
from utils import get_mdl
from utils import get_onehot
from utils import pred_cutoff

############################### Loading annotated data ###############################
print("Loading Data")
data = pd.read_csv('.../climate_dataset.csv')

# replace ERROR lines generated by site title script
data['site_title'] = data['site_title'].str.replace(r'^ERROR.*$', ' ')

data = data.fillna(' ')

# decide final text for training
# data['merged'] = data['message'] + ' ' + data['site_title']
data['merged'] = data['message']

# remove NONE tags (lack of consensus) and duplicates
training_no_none = data.loc[data['Consensus'] != 'NONE']
# training_no_none = training_no_none.drop_duplicates(subset=['merged'])

y = training_no_none['Consensus']
y = y.astype('int64')
y.value_counts()

# clean text and drop duplicate a second time
print("Cleaning Data")
X_data = training_no_none['merged'].apply(clean_text)

y.reset_index(drop=True, inplace=True)
X_data.reset_index(drop=True, inplace=True)

temp = pd.concat([y, X_data], join='outer', axis=1)
temp.columns = ['tags','text']
# temp = temp.drop_duplicates(subset=['text'])

y = temp['tags']
y += 1
X_data = temp['text']

# resample data
# X_data, y = re_sample(X_data, y)

# train test split
X_data, X_val, y, y_val = train_test_split(X_data, y, test_size= 0.1, random_state = 1)
X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size= 0.1, random_state = 1)

# cleaning 
y_train.reset_index(drop=True, inplace=True)
X_train.reset_index(drop=True, inplace=True)

temp = pd.concat([y_train, X_train], join='outer', axis=1)
temp.columns = ['tags','text']
temp = temp.drop_duplicates(subset=['text'])

y_train = temp['tags']
X_train = temp['text']


############################### frequency vectorization ###############################
from scipy.sparse import csr_matrix, hstack
# word
print("Frequency Vectorization")
word_vectorizer = TfidfVectorizer(stop_words= 'english', analyzer='word', use_idf = 1, ngram_range = (1,1), max_features = 5000)
word_vectorizer.fit(X_data)
wtr_vect = word_vectorizer.transform(X_train)
wts_vect = word_vectorizer.transform(X_test)
wva_vect = word_vectorizer.transform(X_val)
# character
char_vectorizer = TfidfVectorizer(stop_words= 'english', analyzer='char', use_idf = 1, ngram_range = (2,6), max_features = 50000)
char_vectorizer.fit(X_data)
ctr_vect = char_vectorizer.transform(X_train)
cts_vect = char_vectorizer.transform(X_test)
cva_vect = char_vectorizer.transform(X_val)

tr_vect = hstack([wtr_vect, ctr_vect]).tocsr()
ts_vect = hstack([wts_vect, cts_vect]).tocsr()
va_vect = hstack([wva_vect, cva_vect]).tocsr()

############################### NB-SVM ###############################
print("Fitting NB-SVM")
m,r = get_mdl(tr_vect,y_train)
pred_nbsvm = m.predict_proba(va_vect.multiply(r))

############################### LSTM ###############################
# parameter values
max_features = 20000
maxlen = 100
batch_size = 32
epochs = 50
num_class = 4
embed_size = 128


print("Tokenizing for LSTM")
tokenizer = text.Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(X_data))
X_tr = tokenizer.texts_to_sequences(X_train)
X_tr = sequence.pad_sequences(X_tr, maxlen=maxlen)
X_ts = tokenizer.texts_to_sequences(X_test)
X_ts = sequence.pad_sequences(X_ts, maxlen=maxlen)
X_va = tokenizer.texts_to_sequences(X_val)
X_va = sequence.pad_sequences(X_va, maxlen=maxlen)

y_tr_one = get_onehot(y_train, num_class)     
y_ts_one = get_onehot(y_test, num_class)  


lstm_input = Input(shape=(maxlen, ))
x = Embedding(max_features, embed_size)(lstm_input)
x = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)
x = GlobalMaxPooling1D()(x)
x = Dropout(0.1)(x)
x = Dense(50, activation="relu")(x)
x = Dropout(0.1)(x)
lstm_output = Dense(num_class, activation="sigmoid")(x)
lstm_model = Model(inputs=lstm_input, outputs=lstm_output)
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


file_path="lstm_final.hdf5"
checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
early = EarlyStopping(monitor="val_loss", mode="min", patience=2)
callbacks_list = [checkpoint, early] #early

lstm_model.fit(X_tr, y_tr_one, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)
lstm_model.fit(X_tr, y_tr_one, batch_size=batch_size, epochs=epochs, validation_data=(X_ts,y_ts_one), callbacks=callbacks_list)
# load best weight
lstm_model.load_weights(file_path)

pred_lstm = lstm_model.predict(X_tr,batch_size=1024,verbose=1)


############################### GRU with Glove ###############################
# path to GloVe
EMBEDDING_FILE = '../glove.twitter.27B.200d.txt'

max_features=50000
maxlen=150
batch_size = 128
epochs = 20
num_class = 4
embed_size=200


print("Tokenizing for GRU")
tokenizer = text.Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(X_data))
X_tr = tokenizer.texts_to_sequences(X_train)
X_tr = sequence.pad_sequences(X_tr, maxlen=maxlen)
X_ts = tokenizer.texts_to_sequences(X_test)
X_ts = sequence.pad_sequences(X_ts, maxlen=maxlen)
X_va = tokenizer.texts_to_sequences(X_val)
X_va = sequence.pad_sequences(X_va, maxlen=maxlen)

print("Loading GloVe Embeddings")
embeddings_index = {}
with open(EMBEDDING_FILE,encoding='utf8') as f:
    for line in f:
        values = line.rstrip().rsplit(' ')
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

word_index = tokenizer.word_index
#prepare embedding matrix
num_words = min(max_features, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, embed_size))
for word, i in word_index.items():
    if i >= max_features:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

y_tr_one = get_onehot(y_train, num_class)
y_ts_one = get_onehot(y_test, num_class) 


gru_input = Input(shape=(maxlen, ))
x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(gru_input)
x = SpatialDropout1D(0.2)(x)
x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)
x = Conv1D(64, kernel_size = 3, padding = "valid", kernel_initializer = "glorot_uniform")(x)
avg_pool = GlobalAveragePooling1D()(x)
max_pool = GlobalMaxPooling1D()(x)
x = concatenate([avg_pool, max_pool]) 
x = Dense(128, activation='relu')(x)
x = Dropout(0.1)(x)
gru_output = Dense(4, activation="sigmoid")(x)
gru_model = Model(gru_input, gru_output)
gru_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])


file_path="gru_final.hdf5"
checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
early = EarlyStopping(monitor="val_loss", mode="min", patience=4)
callbacks_list = [checkpoint, early] #early

gru_model.fit(X_tr, y_tr_one, batch_size=batch_size, epochs=epochs, validation_data=(X_ts,y_ts_one), callbacks=callbacks_list)
# load best weight
gru_model.load_weights(file_path)
pred_gru = gru_model.predict(X_va,batch_size=1024,verbose=1)


############################### LightGBM ###############################
d_train = lgb.Dataset(tr_vect, label=y_train+1)
d_valid = lgb.Dataset(ts_vect, label=y_test+1)
watchlist = [d_train, d_valid]
params = {'learning_rate': 0.2,
            'application': 'multiclass',
            'num_class': 4,
            'num_leaves': 31,
            'verbosity': -1,
            'metric': 'auc',
            'data_random_seed': 2,
            'bagging_fraction': 0.8,
            'feature_fraction': 0.6,
            'nthread': 4,
            'lambda_l1': 1,
            'lambda_l2': 1}

lgb_model = lgb.train(params,
                    train_set=d_train,
                    valid_sets=watchlist,
                    verbose_eval=10)

pred_lgb = lgb_model.predict(va_vect)

############################### ensembly and prediction ###############################
pred_ens = (pred_nbsvm + pred_lstm + pred_gru + pred_lgb)/4

pred_class, prob = pred_cutoff(pred_ens,0,0,0,0)

# pred_class -= 1

print(np.sum(pred_class==3)/pred_class.shape[0])

print(precision_score(y_val, pred_class, average = None))

print(np.mean(precision_score(y_val, pred_class, average = None)[:4]))

print(classification_report(y_val, pred_class))

print(accuracy_score(y_val, pred_class))



pred_class, prob = pred_cutoff(pred_ens,0,0,0,0)

# pred_class -= 1

print(np.sum(pred_class==3)/pred_class.shape[0])

print(precision_score(y, pred_class, average = None))

print(np.mean(precision_score(y, pred_class, average = None)[:4]))

print(classification_report(y, pred_class))

print(accuracy_score(y, pred_class))